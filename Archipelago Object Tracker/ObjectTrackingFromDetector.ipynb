{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import enum\n",
    "import numpy as np\n",
    "import time\n",
    "import cv2\n",
    "import imutils\n",
    "from imutils.video import FPS\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Direction(enum.Enum):\n",
    "  ONBOARD = 1\n",
    "  OFFBOARD= -1\n",
    "  NAN = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class detection_tracker:\n",
    "  \n",
    "  export_data = {}\n",
    "\n",
    "  def __init__(self, capture, frame_num, frame, bounding_box, label):\n",
    "    self.capture = capture\n",
    "    self.first_frame = frame_num\n",
    "    self.tracker = cv2.TrackerMIL_create()\n",
    "    self.tracker.init(frame, bounding_box)\n",
    "    self.label = label\n",
    "    print('Made a tracker for label' + label + '.')\n",
    "  \n",
    "  def tracker_end(self):\n",
    "    self.duration = self.last_frame - self.first_frame\n",
    "    # build export data\n",
    "    start_timestamp = TimestampFromFrame.stampFromFrame(self.first_frame, capture=self.capture)\n",
    "    last_timestamp = TimestampFromFrame.stampFromFrame(self.last_frame, capture=self.capture)\n",
    "    nested_dict = {(start_timestamp, last_timestamp): self.label, 'direction': self.direction}\n",
    "\n",
    "  def update_tracker(self, frame, frame_num):\n",
    "    okay, bounding_box = self.tracker.update(frame)\n",
    "    if okay:\n",
    "      self.last_frame = frame_num\n",
    "      if self.boundingBox:\n",
    "        self.boundingBox = BBox(bounding_box[0], bounding_box[1], bounding_box[2], bounding_box[3])\n",
    "      return True\n",
    "    else:\n",
    "      self.tracker_end()\n",
    "      return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Austin's Timestamp Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimestampFromFrame:\n",
    "  \n",
    "    # Function to get Timestamp from Frame input\n",
    "    def stampFromFrame(frame, capture):\n",
    "        time_stamp = frame / capture.get(cv2.CAP_PROP_FPS)\n",
    "        return time_stamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### John's data export class FisheriesData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FisheriesData:\n",
    "    def __init__(self):\n",
    "        self.species = \"species\"\n",
    "        self.timeStamp = \"time_stamp\"\n",
    "        self.direction = \"direction\"\n",
    "        self.dataFrame = []\n",
    "        self.speciesList = []\n",
    "        self.timeStampList = []\n",
    "        self.directionList = []\n",
    "    \n",
    "    def makeDF(self):\n",
    "        self.dataFrame = pd.DataFrame({self.species:self.speciesList,self.timeStamp:self.timeStampList,self.direction:self.directionList})\n",
    "    \n",
    "    def addData(self, species, timeStamp, status):\n",
    "        self.speciesList.append(species)\n",
    "        self.timeStampList.append(timeStamp)\n",
    "        self.directionList.append(status)\n",
    "\n",
    "    def writeCSV(self):\n",
    "        self.dataFrame.to_csv('output.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "    def writeExcel(self):\n",
    "        self.dataFrame.to_excel('output.xlsx', sheet_name='sheet1', index=False)\n",
    "\n",
    "    def writeXML(self):\n",
    "        self.dataFrame.to_xml('output.xml')\n",
    "\n",
    "    def writeJSON(self):\n",
    "        self.dataFrame.to_json('output.json', orient='records', indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box Class (tracker alt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBox:\n",
    "  x1 = 0\n",
    "  y1 = 0\n",
    "  x2 = 0\n",
    "  y2 = 0\n",
    "  label = \"\"\n",
    "  \n",
    "  # Consturctor for BBox taking x1,x2,y1,y2\n",
    "  # def __init__(self, x1, x2, y1, y2, label):\n",
    "  #   if x1>x2 or y1>y2:\n",
    "  #     raise ValueError(\"Coordinates are invalid\")\n",
    "  #   if label == \"\" or not label:\n",
    "  #     raise ValueError(\"Please include label\")\n",
    "  #   self.x1, self.y1, self.x2, self.y2 = x1, y1, x2, y2\n",
    "  #   self.label = label\n",
    "\n",
    "  # Constructor for BBox taking x,y w,h\n",
    "  # Use for Yolo detections\n",
    "  def __init__(self, x:int, y:int, w:int, h:int, label:str):\n",
    "    if x>(x+w) or y>(y+h):\n",
    "      raise ValueError(\"Coordinates are invalid\")\n",
    "    if label == \"\" or not label:\n",
    "      raise ValueError(\"Please include label\")\n",
    "    self.x1, self.y1, self.x2, self.y2 = x, y, (x+w), (y+h)\n",
    "    self.label = label\n",
    "\n",
    "  # Takes a BBox class instance to compare the overlap area \n",
    "  # and returns an value for overlap area\n",
    "  def intersection_area(self, bbox: object) -> float:\n",
    "    if type(bbox) != self:\n",
    "      raise TypeError(\"BBox should be an instance of a BBox Class\")\n",
    "    dx = min(self.x1, bbox.x1) - max(self.x2, bbox.x2)\n",
    "    dy = min(self.y1, bbox.y1) - max(self.y2, bbox.y2)\n",
    "    if (dx>=0) and (dy>=0):\n",
    "      if self.label == bbox.label:\n",
    "        return dx*dy\n",
    "    return -1\n",
    "\n",
    "  # Takes a BBox and compares x positions \n",
    "  def direction_of_motion(self, bbox: object):\n",
    "    if type(bbox) != self:\n",
    "      raise TypeError(\"BBox should be an instance of a BBox Class\")\n",
    "    dx = self.x1 - bbox.x1\n",
    "    if dx > 0:\n",
    "      return Direction.ONBOARD\n",
    "    elif dx < 0:\n",
    "      return Direction.OFFBOARD\n",
    "    elif dx == 0:\n",
    "      return Direction.NAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yolo Detector modified from object_detector.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame number 1\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1268: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32mh:\\Programing\\Archipelago\\Archipelago-Object-Tracking\\ObjectTrackingFromDetector.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 45>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/Programing/Archipelago/Archipelago-Object-Tracking/ObjectTrackingFromDetector.ipynb#ch0000010?line=113'>114</a>\u001b[0m \t\tcv2\u001b[39m.\u001b[39mputText(image, text, (x, y \u001b[39m-\u001b[39m \u001b[39m5\u001b[39m), cv2\u001b[39m.\u001b[39mFONT_HERSHEY_SIMPLEX,\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/Programing/Archipelago/Archipelago-Object-Tracking/ObjectTrackingFromDetector.ipynb#ch0000010?line=114'>115</a>\u001b[0m \t\t\t\u001b[39m0.5\u001b[39m, color, \u001b[39m2\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/Programing/Archipelago/Archipelago-Object-Tracking/ObjectTrackingFromDetector.ipynb#ch0000010?line=116'>117</a>\u001b[0m \u001b[39m# show the output image\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/h%3A/Programing/Archipelago/Archipelago-Object-Tracking/ObjectTrackingFromDetector.ipynb#ch0000010?line=117'>118</a>\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m\"\u001b[39;49m\u001b[39moutput\u001b[39;49m\u001b[39m\"\u001b[39;49m, image)\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/Programing/Archipelago/Archipelago-Object-Tracking/ObjectTrackingFromDetector.ipynb#ch0000010?line=118'>119</a>\u001b[0m writer\u001b[39m.\u001b[39mwrite(cv2\u001b[39m.\u001b[39mresize(image,frame_size))\n\u001b[0;32m    <a href='vscode-notebook-cell:/h%3A/Programing/Archipelago/Archipelago-Object-Tracking/ObjectTrackingFromDetector.ipynb#ch0000010?line=119'>120</a>\u001b[0m fps\u001b[39m.\u001b[39mupdate()\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.5) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1268: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "INPUT_FILE='fish.avi'\n",
    "OUTPUT_FILE='output2.mp4'\n",
    "LABELS_FILE='Model/obj.names'\n",
    "CONFIG_FILE='Model/yolov4-obj2.cfg'\n",
    "WEIGHTS_FILE='Model/yolov4-obj2_best.weights'\n",
    "CONFIDENCE_THRESHOLD=0.3\n",
    "\n",
    "H=None\n",
    "W=None\n",
    "\n",
    "# capture input video\n",
    "video_capture = cv2.VideoCapture(INPUT_FILE)\n",
    "\n",
    "# get input video's frame size\n",
    "frame_width = int(video_capture.get(3))\n",
    "frame_height = int(video_capture.get(4))\n",
    "frame_size = (frame_width,frame_height)\n",
    "\n",
    "# get input video's fps\n",
    "input_fps = video_capture.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "fps = FPS().start()\n",
    "\n",
    "# fourcc = cv2.VideoWriter_fourcc(*\"MJPG\") # for avi\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\") # for mp4\n",
    "writer = cv2.VideoWriter(OUTPUT_FILE, fourcc, input_fps, frame_size, True)\n",
    "\n",
    "# make Labels with labels_file\n",
    "LABELS = open(LABELS_FILE).read().strip().split(\"\\n\")\n",
    "\n",
    "# set random color for labels and bounding boxes\n",
    "np.random.seed(4)\n",
    "COLORS = np.random.randint(0, 255, size=(len(LABELS), 3), dtype=\"uint8\")\n",
    "\n",
    "# load the YOLO network model with config and weights file\n",
    "net = cv2.dnn.readNetFromDarknet(CONFIG_FILE, WEIGHTS_FILE)\n",
    "\n",
    "# determine only the *output* layer names that we need from YOLO\n",
    "ln = net.getLayerNames()\n",
    "ln = [ln[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "cnt =0\n",
    "\n",
    "# iterate through video frames\n",
    "while True:\n",
    "\tcnt+=1\n",
    "\tprint (\"Frame number\", cnt)\n",
    "\tok, image = video_capture.read()\n",
    "\tif not ok:\n",
    "\t\tbreak\n",
    "\t# transform image into a blob\n",
    "\tblob = cv2.dnn.blobFromImage(image, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n",
    "\tnet.setInput(blob)\n",
    "\tif W is None or H is None:\n",
    "\t\t(H, W) = image.shape[:2]\n",
    "\tlayerOutputs = net.forward(ln)\n",
    "\n",
    "\t# initialize our lists of detected bounding boxes, confidences, and\n",
    "\t# class IDs, respectively\n",
    "\tboxes = []\n",
    "\tconfidences = []\n",
    "\tclassIDs = []\n",
    "\n",
    "\t# loop over each of the layer outputs\n",
    "\tfor output in layerOutputs:\n",
    "\t\t# loop over each of the detections\n",
    "\t\tfor detection in output:\n",
    "\t\t\t# extract the class ID and confidence (i.e., probability) of\n",
    "\t\t\t# the current object detection\n",
    "\t\t\tscores = detection[5:]\n",
    "\t\t\tclassID = np.argmax(scores)\n",
    "\t\t\tconfidence = scores[classID]\n",
    "\n",
    "\t\t\t# filter out weak predictions by ensuring the detected\n",
    "\t\t\t# probability is greater than the minimum probability\n",
    "\t\t\tif confidence > CONFIDENCE_THRESHOLD:\n",
    "\t\t\t\t# scale the bounding box coordinates back relative to the\n",
    "\t\t\t\t# size of the image, keeping in mind that YOLO actually\n",
    "\t\t\t\t# returns the center (x, y)-coordinates of the bounding\n",
    "\t\t\t\t# box followed by the boxes' width and height\n",
    "\t\t\t\tbox = detection[0:4] * np.array([W, H, W, H])\n",
    "\t\t\t\t(centerX, centerY, width, height) = box.astype(\"int\")\n",
    "\n",
    "\t\t\t\t# use the center (x, y)-coordinates to derive the top and\n",
    "\t\t\t\t# and left corner of the bounding box\n",
    "\t\t\t\tx = int(centerX - (width / 2))\n",
    "\t\t\t\ty = int(centerY - (height / 2))\n",
    "\n",
    "\t\t\t\t# Add a good confidence detection to the detection tracker\n",
    "\t\t\t\tdetection_tracker(video_capture, cnt, image, (x, y, width, height), LABELS[classID])\n",
    "\n",
    "\t\t\t\t# update our list of bounding box coordinates, confidences,\n",
    "\t\t\t\t# and class IDs\n",
    "\t\t\t\tboxes.append([x, y, int(width), int(height)])\n",
    "\t\t\t\tconfidences.append(float(confidence))\n",
    "\t\t\t\tclassIDs.append(classID)\n",
    "\n",
    "\t# apply non-maxima suppression to suppress weak, overlapping bounding\n",
    "\t# boxes\n",
    "\tidxs = cv2.dnn.NMSBoxes(boxes, confidences, CONFIDENCE_THRESHOLD,\n",
    "\t\tCONFIDENCE_THRESHOLD)\n",
    "\n",
    "\t# ensure at least one detection exists\n",
    "\tif len(idxs) > 0:\n",
    "\t\t# loop over the indexes we are keeping\n",
    "\t\tfor i in idxs.flatten():\n",
    "\t\t\t# extract the bounding box coordinates\n",
    "\t\t\t(x, y) = (boxes[i][0], boxes[i][1])\n",
    "\t\t\t(w, h) = (boxes[i][2], boxes[i][3])\n",
    "\n",
    "\t\t\tcolor = [int(c) for c in COLORS[classIDs[i]]]\n",
    "\n",
    "\t\t\tcv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "\t\t\ttext = \"{}: {:.4f}\".format(LABELS[classIDs[i]], confidences[i])\n",
    "\t\t\tcv2.putText(image, text, (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "\t\t\t\t0.5, color, 2)\n",
    "\n",
    "\t# show the output image\n",
    "\tcv2.imshow(\"output\", image)\n",
    "\twriter.write(cv2.resize(image,frame_size))\n",
    "\tfps.update()\n",
    "\tkey = cv2.waitKey(1) & 0xFF\n",
    "\tif key == ord(\"q\"):\n",
    "\t\tbreak\n",
    "\n",
    "fps.stop()\n",
    "\n",
    "print(\"[INFO] elapsed time: {:.2f}\".format(fps.elapsed()))\n",
    "print(\"[INFO] approx. FPS: {:.2f}\".format(fps.fps()))\n",
    "\n",
    "# do a bit of cleanup\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# release the file pointers\n",
    "print(\"[INFO] cleaning up...\")\n",
    "writer.release()\n",
    "video_capture.release()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dfd8b5841c8fdbae2936e39ec67041aa4f03e46e0e941bdc94af56c5f1f7a941"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('OpenCV')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
